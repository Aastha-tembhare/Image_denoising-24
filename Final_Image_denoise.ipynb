{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLK1ryDZ+gLZamxFP1kznD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aastha-tembhare/Image_denoising-24/blob/main/Final_Image_denoise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po3lxOAxwOUx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.augmentations as augmentations\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SCPA Block\n",
        "class SCPA(nn.Module):\n",
        "    def __init__(model,in_ch,out_ch):\n",
        "        super(SCPA,model).__init__()\n",
        "        model.conv1_branch1 = nn.Conv2d(in_ch, out_ch, k_size=1, strd=1, pad=0)\n",
        "        model.conv2_branch1 = nn.Conv2d(in_ch, out_ch, k_size=3, strd=1, pad=1)\n",
        "        model.sigmoid = nn.Sigmoid()\n",
        "        model.conv1_branch2 = nn.Conv2d(in_ch, out_ch, k_size=1, strd=1, pad=0)\n",
        "        model.conv2_branch2 = nn.Conv2d(in_ch, out_ch, k_size=1, strd=1, pad=0)\n",
        "        model.conv3_branch2 = nn.Conv2d(in_ch, out_ch, k_size=3, strd=1, pad=1)\n",
        "        model.conv4_branch2 = nn.Conv2d(in_ch, out_ch, k_size=3, strd=1, pad=1)\n",
        "        model.final_conv =    nn.Conv2d(in_ch, out_ch, k_size=1, strd=1, pad=0)\n",
        "    def forward(model,input_tensor):\n",
        "        # branch 1\n",
        "        branch1 = model.conv1_branch1(input_tensor)\n",
        "        branch1 = model.conv2_branch1(branch1)\n",
        "        # branch 2\n",
        "        branch2 = model.conv1_branch2(input_tensor)\n",
        "        branch2a = model.conv2_branch2(branch2)\n",
        "        branch2a = model.sigmoid(branch2a)\n",
        "        branch2b = model.conv3_branch2(branch2)\n",
        "        branch2 = branch2a*branch2b\n",
        "        branch2 = model.conv4_branch2(branch2)\n",
        "\n",
        "        #combining branch 1 and branch 2\n",
        "        output = branch2 + branch1\n",
        "\n",
        "        #final convolutional layer and add it to the orignal input\n",
        "        final_conv = model.final_conv(output)\n",
        "        SCPA_output = final_conv + input_tensor\n",
        "        return SCPA_output\n",
        "\n"
      ],
      "metadata": {
        "id": "8ecNZRZowglG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CoordConv Bloack\n",
        "class CoordConv(nn.Module):\n",
        "    def __init__(model, in_ch, out_ch):\n",
        "        super(CoordConv, model).__init__()\n",
        "        model.conv = nn.Conv2d(in_ch+2, out_ch, k_size=3, pad=1)\n",
        "\n",
        "    def forward(model, input_tensor):\n",
        "        batch_size, _, height, width = input_tensor.size()\n",
        "        input_tensorinput_tensor = torch.arange(width).repeat(height, 1)\n",
        "        yy = torch.arange(height).view(-1, 1).repeat(1, width)\n",
        "        input_tensorinput_tensor = input_tensorinput_tensor.float() / (width - 1)\n",
        "        yy = yy.float() / (height - 1)\n",
        "        input_tensorinput_tensor = input_tensorinput_tensor.repeat(batch_size, 1, 1).unsqueeze(1)\n",
        "        yy = yy.repeat(batch_size, 1, 1).unsqueeze(1)\n",
        "        if input_tensor.is_cuda:\n",
        "            input_tensorinput_tensor = input_tensorinput_tensor.cuda()\n",
        "            yy = yy.cuda()\n",
        "        # adding input_tensor and y cordinate to RGB channel\n",
        "        input_tensor = torch.cat([input_tensor, input_tensorinput_tensor, yy], dim=1)\n",
        "        input_tensor = model.conv(input_tensor)\n",
        "        return input_tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "p53QCbimxIIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InvResBlock(nn.Module):\n",
        "    def __init__(model, in_ch, mid_channels, out_ch):\n",
        "        super(InvResBlock, model).__init__()\n",
        "        model.conv1 = nn.Conv2d(in_ch, mid_channels, k_size=1, pad=0)\n",
        "        model.bn1 = nn.BatchNorm2d(mid_channels)\n",
        "        model.relu = nn.ReLU()\n",
        "        model.conv2 = nn.Conv2d(mid_channels, mid_channels, k_size=3, pad=1)\n",
        "        model.bn2 = nn.BatchNorm2d(mid_channels)\n",
        "        model.conv3 = nn.Conv2d(mid_channels, out_ch, k_size=1, pad=0)\n",
        "        model.bn3 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(model, input_tensor):\n",
        "        identity = input_tensor\n",
        "\n",
        "        out = model.conv1(input_tensor)\n",
        "        out = model.bn1(out)\n",
        "        out = model.relu(out)\n",
        "\n",
        "        out = model.conv2(out)\n",
        "        out = model.bn2(out)\n",
        "        out = model.relu(out)\n",
        "\n",
        "        out = model.conv3(out)\n",
        "        out = model.bn3(out)\n",
        "\n",
        "        out += identity\n",
        "\n",
        "        out = model.relu(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "-9LIughTxSjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttBlock(nn.Module):\n",
        "    def __init__(model, in_ch):\n",
        "        super(AttBlock, model).__init__()\n",
        "        model.conv1 = nn.Conv2d(in_ch, 1, k_size=1)\n",
        "        model.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(model, input_tensor):\n",
        "        # Calculate attention weights\n",
        "        attn_weights = model.conv1(input_tensor)\n",
        "        attn_weights = model.sigmoid(attn_weights)\n",
        "        # Apply attention to input features\n",
        "        input_tensor_attention = input_tensor * attn_weights\n",
        "        return input_tensor_attention\n"
      ],
      "metadata": {
        "id": "TXvNSTB1xY4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def double_conv(in_c,out_c):\n",
        "    conv = nn.Sequential(\n",
        "    nn.Conv2d(in_c,out_c,k_size=3,pad =1), # pad is added on my own\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(out_c,out_c,k_size=3,pad =1 ),\n",
        "    nn.ReLU(inplace=True)\n",
        "    )\n",
        "    return conv\n",
        "def crop_img(tensor, target_tensor):\n",
        "    _, _, h, w = target_tensor.size()\n",
        "    return tensor[:, :, :h, :w]\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(model):\n",
        "        super(Unet, model).__init__()\n",
        "        model.mainput_tensor_pool_2input_tensor2 = nn.Mainput_tensorPool2d(k_size=2, strd=2)\n",
        "        model.down_conv_1 = double_conv(3, 64)\n",
        "        model.down_conv_2 = double_conv(64, 128)\n",
        "        model.down_conv_3 = double_conv(128, 256)\n",
        "        model.down_conv_4 = double_conv(256, 512)\n",
        "        model.down_conv_5 = double_conv(512, 1024)\n",
        "\n",
        "        model.up_trans_1 = nn.ConvTranspose2d(in_ch=1024, out_ch=512, k_size=2, strd=2)\n",
        "        model.up_conv_1 = double_conv(1024, 512)\n",
        "        model.up_trans_2 = nn.ConvTranspose2d(in_ch=512, out_ch=256, k_size=2, strd=2)\n",
        "        model.up_conv_2 = double_conv(512, 256)\n",
        "        model.up_trans_3 = nn.ConvTranspose2d(in_ch=256, out_ch=128, k_size=2, strd=2)\n",
        "        model.up_conv_3 = double_conv(256, 128)\n",
        "        model.up_trans_4 = nn.ConvTranspose2d(in_ch=128, out_ch=64, k_size=2, strd=2)\n",
        "        model.up_conv_4 = double_conv(128, 64)\n",
        "        model.out = nn.Conv2d(in_ch=64, out_ch=3, k_size=1)\n",
        "\n",
        "    def forward(model, img):\n",
        "        input_tensor1 = model.down_conv_1(img)\n",
        "        input_tensor2 = model.mainput_tensor_pool_2input_tensor2(input_tensor1)  # skip\n",
        "        input_tensor3 = model.down_conv_2(input_tensor2)\n",
        "        input_tensor4 = model.mainput_tensor_pool_2input_tensor2(input_tensor3)  # skip\n",
        "        input_tensor5 = model.down_conv_3(input_tensor4)\n",
        "        input_tensor6 = model.mainput_tensor_pool_2input_tensor2(input_tensor5)  # skip\n",
        "        input_tensor7 = model.down_conv_4(input_tensor6)\n",
        "        input_tensor8 = model.mainput_tensor_pool_2input_tensor2(input_tensor7)  # skip\n",
        "        input_tensor9 = model.down_conv_5(input_tensor8)\n",
        "\n",
        "        input_tensor = model.up_trans_1(input_tensor9)\n",
        "        y = crop_img(input_tensor7, input_tensor)\n",
        "        input_tensor = model.up_conv_1(torch.cat([input_tensor, y], 1))\n",
        "\n",
        "        input_tensor = model.up_trans_2(input_tensor)\n",
        "        y = crop_img(input_tensor5, input_tensor)\n",
        "        input_tensor = model.up_conv_2(torch.cat([input_tensor, y], 1))\n",
        "\n",
        "        input_tensor = model.up_trans_3(input_tensor)\n",
        "        y = crop_img(input_tensor3, input_tensor)\n",
        "        input_tensor = model.up_conv_3(torch.cat([input_tensor, y], 1))\n",
        "\n",
        "        input_tensor = model.up_trans_4(input_tensor)\n",
        "        y = crop_img(input_tensor1, input_tensor)\n",
        "        input_tensor = model.up_conv_4(torch.cat([input_tensor, y], 1))\n",
        "\n",
        "        input_tensor = model.out(input_tensor)\n",
        "        return input_tensor\n"
      ],
      "metadata": {
        "id": "0K6-pWzfyBvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Denoising branch\n",
        "'''\n",
        "convolutional block -> 4 inv residual block -> attention block -> convolution block\n",
        "'''\n",
        "class DenoiseBranch(nn.Module):\n",
        "    def __init__(model,in_ch=3,out_ch=3):\n",
        "        super(DenoiseBranch,model).__init__()\n",
        "        model.conv_1 = nn.Conv2d(in_ch=in_ch,out_ch =256,k_size = 3,pad =1)\n",
        "        model.inv_1 = InvResBlock(in_ch=256, mid_channels=128, out_ch=256)\n",
        "        model.inv_2 = InvResBlock(in_ch=256, mid_channels=128, out_ch=256)\n",
        "        model.inv_3 = InvResBlock(in_ch=256, mid_channels=128, out_ch=256)\n",
        "        model.inv_4 = InvResBlock(in_ch=256, mid_channels=128, out_ch=256)\n",
        "        model.attention = AttBlock(in_ch = 256)\n",
        "        model.conv_2 = nn.Conv2d(in_ch=256,out_ch = 3,k_size = 3,pad =1)\n",
        "\n",
        "    def forward(model,input_tensor):\n",
        "        input_tensor = model.conv_1(input_tensor)\n",
        "        input_tensor = model.inv_1(input_tensor)\n",
        "        input_tensor = model.inv_2(input_tensor)\n",
        "        input_tensor = model.inv_3(input_tensor)\n",
        "        input_tensor = model.inv_4(input_tensor)\n",
        "        input_tensor = model.attention(input_tensor)\n",
        "        input_tensor = model.conv_2(input_tensor)\n",
        "        return input_tensor\n"
      ],
      "metadata": {
        "id": "mOV8a-WTyKXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SCPA_Branch(nn.Module):\n",
        "    def __init__(model,in_ch=3,out_ch=3):\n",
        "        super(SCPA_Branch,model).__init__()\n",
        "        model.coord_layer = CoordConv(in_ch =3,out_ch = 5)\n",
        "        model.SCPA_1 = SCPA(in_ch = 5, out_ch =5)\n",
        "        model.SCPA_2 = SCPA(in_ch = 5, out_ch =5)\n",
        "        model.SCPA_3 = SCPA(in_ch = 5, out_ch =5)\n",
        "        model.SCPA_4 = SCPA(in_ch = 5, out_ch =5)\n",
        "        model.SCPA_5 = SCPA(in_ch = 5, out_ch =5)\n",
        "        model.conv_layer =nn.Conv2d(in_ch=5,out_ch =3,k_size = 3,pad =1)\n",
        "\n",
        "    def forward(model,input_tensor):\n",
        "        input_tensor = model.coord_layer(input_tensor)\n",
        "        input_tensor = model.SCPA_1(input_tensor)\n",
        "        input_tensor = model.SCPA_2(input_tensor)\n",
        "        input_tensor = model.SCPA_3(input_tensor)\n",
        "        input_tensor = model.SCPA_4(input_tensor)\n",
        "        input_tensor = model.SCPA_5(input_tensor)\n",
        "        input_tensor = model.conv_layer(input_tensor)\n",
        "        return input_tensor\n"
      ],
      "metadata": {
        "id": "gCfjb1PfySZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LowLightModel(nn.Module):\n",
        "    def __init__(model,in_ch=3,out_ch=3):\n",
        "        super(LowLightModel,model).__init__()\n",
        "        model.SCPA_branch = SCPA_Branch(in_ch=3,out_ch=3)\n",
        "        model.Denoiser = DenoiseBranch(in_ch=3,out_ch=3)\n",
        "        model.Unet = Unet()\n",
        "        model.Conv = nn.Conv2d(in_ch=3,out_ch =3,k_size = 3,pad =1)\n",
        "\n",
        "    def forward(model,input_tensor):\n",
        "        # denoiser branch\n",
        "        denoised = model.Denoiser(input_tensor)\n",
        "\n",
        "        #SCPA branch\n",
        "        SCPA = model.SCPA_branch(input_tensor)\n",
        "        Unet_input = SCPA + input_tensor\n",
        "        Unet_output = model.Unet(Unet_input)\n",
        "        # pad\n",
        "        diff = Unet_input.size(3) - Unet_output.size(3)\n",
        "        pad_left = diff // 2\n",
        "        pad_right = diff - pad_left\n",
        "\n",
        "        # Pad Unet_output\n",
        "        Unet_output = F.pad(Unet_output, (pad_left, pad_right, 0, 0))\n",
        "\n",
        "        output = model.Conv(Unet_output)\n",
        "        final_output = output + denoised\n",
        "        return final_output\n"
      ],
      "metadata": {
        "id": "kfyHjA_U_Obu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LowLightDataset(Dataset):\n",
        "    def __init__(model, low_light_dir, bright_light_dir, augmentation=None):\n",
        "        model.low_light_dir = low_light_dir\n",
        "        model.bright_light_dir = bright_light_dir\n",
        "        model.low_imgs = sorted(os.listdir(low_light_dir))\n",
        "        model.high_imgs = sorted(os.listdir(bright_light_dir))\n",
        "        model.augmentation = augmentation\n",
        "\n",
        "    def __len__(model):\n",
        "        return len(model.low_imgs)\n",
        "\n",
        "    def __getitem__(model, idinput_tensor):\n",
        "        low_img_path = os.path.join(model.low_light_dir, model.low_imgs[idinput_tensor])\n",
        "        high_img_path = os.path.join(model.bright_light_dir, model.high_imgs[idinput_tensor])\n",
        "        low_img = Image.open(low_img_path).convert(\"RGB\")\n",
        "        high_img = Image.open(high_img_path).convert(\"RGB\")\n",
        "\n",
        "        if model.augmentation:\n",
        "            low_img = model.augmentation(low_img)\n",
        "            high_img = model.augmentation(high_img)\n",
        "\n",
        "        return low_img, high_img\n",
        "\n",
        "augmentation = augmentations.Compose([\n",
        "    augmentations.ToTensor(),\n",
        "])\n"
      ],
      "metadata": {
        "id": "rSMMPkZt_Y9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def psnr(img1, img2):\n",
        "    mse = F.mse_loss(img1, img2)\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "    piinput_tensorel_mainput_tensor = 1.0\n",
        "    return 20 * torch.log10(piinput_tensorel_mainput_tensor / torch.sqrt(mse))\n"
      ],
      "metadata": {
        "id": "5241oGCW_jKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "jMGUqaOq_nMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Path to the folder in Google Drive\n",
        "source_folder = '/content/drive/My Drive/LOLdataset/'\n",
        "\n",
        "# Path where you want to copy the folder\n",
        "destination_folder = '/content/LOLdataset/'\n",
        "\n",
        "# Create target directory if it doesn't einput_tensorist\n",
        "if not os.path.einput_tensorists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "\n",
        "# Copy the entire folder\n",
        "shutil.copytree(source_folder, destination_folder, dirs_einput_tensorist_ok=True)\n",
        "\n",
        "print(f'Folder copied to: {destination_folder}')\n"
      ],
      "metadata": {
        "id": "p7-DhWDJ_qA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Dataset paths\n",
        "train_low_dir = '/content/LOLdataset/our485/low'\n",
        "train_high_dir = '/content/LOLdataset/our485/high'\n",
        "val_low_dir = '/content/LOLdataset/eval15/low'\n",
        "val_high_dir = '/content/LOLdataset/eval15/high'\n"
      ],
      "metadata": {
        "id": "90pHF-GT_6dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets and dataloaders\n",
        "train_dataset = LowLightDataset(train_low_dir, train_high_dir, augmentation=augmentation)\n",
        "val_dataset = LowLightDataset(val_low_dir, val_high_dir, augmentation=augmentation)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n"
      ],
      "metadata": {
        "id": "4V-4z5n9_9O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.tensor([torch.einput_tensorp(torch.tensor(-(input_tensor - window_size // 2) ** 2 / float(2 * sigma ** 2))) for input_tensor in range(window_size)], dtype=torch.float32)\n",
        "    return gauss / gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = _2D_window.einput_tensorpand(channel, 1, window_size, window_size).contiguous()\n",
        "    return window\n",
        "\n",
        "def ssim(img1, img2, window_size=11, size_average=True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel).to(img1.device)\n",
        "\n",
        "    mu1 = F.conv2d(img1, window, pad=window_size // 2, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, pad=window_size // 2, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, pad=window_size // 2, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, pad=window_size // 2, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, pad=window_size // 2, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01 ** 2\n",
        "    C2 = 0.03 ** 2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    return ssim_map.mean() if size_average else ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "class SSIMLoss(nn.Module):\n",
        "    def __init__(model, window_size=11, size_average=True):\n",
        "        super(SSIMLoss, model).__init__()\n",
        "        model.window_size = window_size\n",
        "        model.size_average = size_average\n",
        "\n",
        "    def forward(model, img1, img2):\n",
        "        return 1 - ssim(img1, img2, model.window_size, model.size_average)\n",
        "\n",
        "# Combined Loss Function\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(model):\n",
        "        super(CombinedLoss, model).__init__()\n",
        "        model.ssim_loss = SSIMLoss()\n",
        "        model.l1_loss = nn.L1Loss()\n",
        "\n",
        "    def forward(model, output, target):\n",
        "        # L1 loss\n",
        "        l1_loss = model.l1_loss(output, target)\n",
        "\n",
        "        # SSIM loss\n",
        "        ssim_loss = model.ssim_loss(output, target)\n",
        "\n",
        "        # Gradient loss\n",
        "        grad_loss = model.gradient_loss(output, target)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = 0.1 * ssim_loss + l1_loss + grad_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def gradient_loss(model, output, target):\n",
        "        # Compute gradients\n",
        "        output_grad_input_tensor = torch.abs(output[:, :, :, :-1] - output[:, :, :, 1:])\n",
        "        output_grad_y = torch.abs(output[:, :, :-1, :] - output[:, :, 1:, :])\n",
        "        target_grad_input_tensor = torch.abs(target[:, :, :, :-1] - target[:, :, :, 1:])\n",
        "        target_grad_y = torch.abs(target[:, :, :-1, :] - target[:, :, 1:, :])\n",
        "\n",
        "        # Compute gradient loss\n",
        "        grad_loss_input_tensor = F.l1_loss(output_grad_input_tensor, target_grad_input_tensor)\n",
        "        grad_loss_y = F.l1_loss(output_grad_y, target_grad_y)\n",
        "\n",
        "        return grad_loss_input_tensor + grad_loss_y\n"
      ],
      "metadata": {
        "id": "8sX099a_ABYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = LowLightModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = CombinedLoss()"
      ],
      "metadata": {
        "id": "4yCIlwrIGDNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Epochs = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(Epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for low_img, high_img in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{Epochs}] Training\", leave=False):\n",
        "        low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(low_img)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, high_img)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_psnr = 0.0\n",
        "    with torch.no_grad():\n",
        "        for low_img, high_img in val_loader:\n",
        "            low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "            output = model(low_img)\n",
        "            val_psnr += psnr(output, high_img).item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    val_psnr /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{Epochs}], Loss: {train_loss:.4f}, PSNR: {val_psnr:.2f} dB\")\n"
      ],
      "metadata": {
        "id": "gs7K7p8mAQ3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_epochs = 15  # New number of epochs to run\n",
        "start_epoch = 40\n",
        "total_epochs = start_epoch + new_epochs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(start_epoch, total_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for low_img, high_img in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Training\", leave=False):\n",
        "        low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(low_img)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, high_img)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_psnr = 0.0\n",
        "    with torch.no_grad():\n",
        "        for low_img, high_img in tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Validation\", leave=False):\n",
        "            low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "            output = model(low_img)\n",
        "            val_psnr += psnr(output, high_img).item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    val_psnr /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{total_epochs}], Loss: {train_loss:.4f}, PSNR: {val_psnr:.2f} dB\")\n",
        "\n",
        "torch.save(model.state_dict(), 'model_55epochs.pth')\n"
      ],
      "metadata": {
        "id": "xr5pvbgqAW1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Einput_tensorample settings\n",
        "new_epochs = 15  # New number of epochs to run\n",
        "start_epoch = 40\n",
        "total_epochs = start_epoch + new_epochs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(start_epoch, total_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for low_img, high_img in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Training\", leave=False):\n",
        "        low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(low_img)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, high_img)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_psnr = 0.0\n",
        "    with torch.no_grad():\n",
        "        for low_img, high_img in tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Validation\", leave=False):\n",
        "            low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "            output = model(low_img)\n",
        "            val_psnr += psnr(output, high_img).item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    val_psnr /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{total_epochs}], Loss: {train_loss:.4f}, PSNR: {val_psnr:.2f} dB\")\n",
        "\n",
        "    # Save the model after each epoch\n",
        "    save_path = f'model_epoch_{epoch+1}.pth'\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved successfully at {save_path}\")\n"
      ],
      "metadata": {
        "id": "ly7_YDfqAd9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0004)\n",
        "new_epochs = 5 # New number of epochs to run\n",
        "start_epoch = 70\n",
        "total_epochs = start_epoch + new_epochs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(start_epoch, total_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    # Wrap train_loader with tqdm\n",
        "    for low_img, high_img in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Training\", leave=False):\n",
        "        low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(low_img)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, high_img)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_psnr = 0.0\n",
        "    # Wrap val_loader with tqdm\n",
        "    with torch.no_grad():\n",
        "        for low_img, high_img in tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Validation\", leave=False):\n",
        "            low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "            output = model(low_img)\n",
        "            val_psnr += psnr(output, high_img).item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    val_psnr /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{total_epochs}], Loss: {train_loss:.4f}, PSNR: {val_psnr:.2f} dB\")\n",
        "\n",
        "# Save the state after additional training\n",
        "torch.save(model.state_dict(), 'model_75epochs.pth')\n"
      ],
      "metadata": {
        "id": "xZQku7F9BZhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Setting up the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0004)\n",
        "\n",
        "# Training parameters\n",
        "new_epochs = 5  # New number of epochs to run\n",
        "start_epoch = 70\n",
        "total_epochs = start_epoch + new_epochs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(start_epoch, total_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    # Wrap train_loader with tqdm for progress bar\n",
        "    for low_img, high_img in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Training\", leave=False):\n",
        "        low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(low_img)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, high_img)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_psnr = 0.0\n",
        "    # Wrap val_loader with tqdm for progress bar\n",
        "    with torch.no_grad():\n",
        "        for low_img, high_img in tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Validation\", leave=False):\n",
        "            low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "            output = model(low_img)\n",
        "            val_psnr += psnr(output, high_img).item()\n",
        "\n",
        "    # Average losses and PSNR for the epoch\n",
        "    train_loss /= len(train_loader)\n",
        "    val_psnr /= len(val_loader)\n",
        "\n",
        "    # Log training and validation results\n",
        "    print(f\"Epoch [{epoch+1}/{total_epochs}], Loss: {train_loss:.4f}, PSNR: {val_psnr:.2f} dB\")\n",
        "\n",
        "    # Save the model state after each epoch\n",
        "    save_path = f'model_epoch_{epoch+1}.pth'\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved successfully at {save_path}\")\n",
        "\n",
        "# Save the final state after all additional training\n",
        "final_save_path = 'model_75epochs.pth'\n",
        "torch.save(model.state_dict(), final_save_path)\n",
        "print(f\"Final model saved successfully at {final_save_path}\")\n"
      ],
      "metadata": {
        "id": "GAZOUOKNCJ1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.00002)\n",
        "new_epochs = 5 # New number of epochs to run\n",
        "start_epoch = 100\n",
        "total_epochs = start_epoch + new_epochs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(start_epoch, total_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    # Wrap train_loader with tqdm\n",
        "    for low_img, high_img in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Training\", leave=False):\n",
        "        low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(low_img)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, high_img)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_psnr = 0.0\n",
        "    # Wrap val_loader with tqdm\n",
        "    with torch.no_grad():\n",
        "        for low_img, high_img in tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Validation\", leave=False):\n",
        "            low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "            output = model(low_img)\n",
        "            val_psnr += psnr(output, high_img).item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    val_psnr /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{total_epochs}], Loss: {train_loss:.4f}, PSNR: {val_psnr:.2f} dB\")\n",
        "\n",
        "# Save the state after additional training\n",
        "torch.save(model.state_dict(), 'model_100epochs.pth')\n",
        "\n"
      ],
      "metadata": {
        "id": "xIu_KHZBC9Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set up the optimizer with a very low learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00002)\n",
        "\n",
        "# Define training parameters\n",
        "new_epochs = 5  # New number of epochs to run\n",
        "start_epoch = 100\n",
        "total_epochs = start_epoch + new_epochs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(start_epoch, total_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    # Wrap train_loader with tqdm for progress feedback\n",
        "    for low_img, high_img in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Training\", leave=False):\n",
        "        low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(low_img)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, high_img)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_psnr = 0.0\n",
        "    # Wrap val_loader with tqdm for validation progress\n",
        "    with torch.no_grad():\n",
        "        for low_img, high_img in tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Validation\", leave=False):\n",
        "            low_img, high_img = low_img.to(device), high_img.to(device)\n",
        "            output = model(low_img)\n",
        "            val_psnr += psnr(output, high_img).item()\n",
        "\n",
        "    # Calculate average loss and PSNR for the epoch\n",
        "    train_loss /= len(train_loader)\n",
        "    val_psnr /= len(val_loader)\n",
        "\n",
        "    # Log training and validation results\n",
        "    print(f\"Epoch [{epoch+1}/{total_epochs}], Loss: {train_loss:.4f}, PSNR: {val_psnr:.2f} dB\")\n",
        "\n",
        "    # Save the model state after each epoch\n",
        "    save_path = f'model_epoch_{epoch+1}.pth'\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved successfully at {save_path}\")\n",
        "\n",
        "# Save the final state after all additional training\n",
        "final_save_path = 'model_105epochs.pth'\n",
        "torch.save(model.state_dict(), final_save_path)\n",
        "print(f\"Final model saved successfully at {final_save_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UlG5RUf2DDIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "augmentation = augmentations.Compose([\n",
        "    augmentations.Resize((400, 600)),\n",
        "    augmentations.ToTensor()\n",
        "])\n",
        "\n",
        "input_img_path = '/content/Screenshot 2024-06-05 214421.png'\n",
        "input_img = Image.open(input_img_path).convert('RGB')\n",
        "input_tensor = augmentation(input_img).unsqueeze(0)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "input_tensor = input_tensor.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    enhanced_tensor = model(input_tensor)\n",
        "\n",
        "    enhanced_tensor = enhanced_tensor.cpu()\n",
        "enhanced_img = augmentations.ToPILImage()(enhanced_tensor.squeeze())\n",
        "plt.imshow(enhanced_img)\n",
        "plt.title('Enhanced Image')\n",
        "plt.ainput_tensoris('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TBjdOCs9EV7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(\"/content/Screenshot 2024-06-05 214421.png\")\n",
        "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img_rgb)\n",
        "plt.ainput_tensoris('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l4LLdrzqEfEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_100epoch.pth')"
      ],
      "metadata": {
        "id": "nTBNtqM-EiJB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}